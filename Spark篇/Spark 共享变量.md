# Spark 共享变量

有时候，需要在多个任务之间共享变量，或者在任务（Task）和任务控制节点（Driver Program）之间共享变量。广播变量（broadcast variables）和累加器（accumulators）应运而生。

广播变量用来把变量在所有节点的内存之间进行共享。累加器则支持在所有不同节点之间进行累加计算（比如计数或者求和）。

#### 广播变量

广播变量允许程序开发人员在每个机器上缓存一个只读的变量，而不是为机器上的每个任务都生成一个副本。通过这种方式，就可以非常高效的给每个节点提供一个大的输入数据集的副本。Spark的“动作”操作会跨越多个阶段。对于每个阶段内所有任务所需要的公共数据。Spark都会自动进行广播。通过广播方式进行传播的变量。会经过序列化，然后在被任务使用时再进行反序列化。这意味着，显示地创建广播变量只有在下面的情形中是有用的。当跨越多个阶段的那些任务需要相同的数据，或者当反序列化方式对数据进行缓存是非常重要的。

可以通过调用SparkContent.broadcast(v)来从一个普通变量v中创建一个广播变量。这个广播变量就是对普通变量v的一个包装器，通过调用value方法就可以获得这个广播变量的值。

```python
broadcastVar = sc.broadcast([1,2,3])
broadcastVar.value
```

这个广播变量被创建以后，那么在集群中的任何函数中，都应该使用广播变量broadcastVar的值，而不是使用v的值，这样就不会把v重复分发到这些节点上。此外，一旦广播变量创建后，普通变量v的值就不能再发生修改，从而确保所有节点都获得这个广播变量的相同的值。

#### 累加器

累加器是仅仅被相关操作累加的变量，通常可以被用来实现计数器（counter）和求和（sum）。Spark原生地支持数值型的累加器，程序开发人员可以编写对新类型的支持。如果创建累加器时指定了名字，则可以在Spark UI界面看到，这有利于理解每个执行阶段的进程。

一个数值型的累加器，可以通过调用SparkContent.accumulator()来创建。运行在集群中的任务，就可以使用add方法来把书值累加到累加器上，但是，这些任务只能做累加操作，不能读取累加器的值，只有任务控制节点可以使用value方法来读取累加器的值

```python
accum = sc.accumulator(0)
sc.parallelize([1,2,3,4]).foreach(lambda x : accum.add(x))
accum.value
```



